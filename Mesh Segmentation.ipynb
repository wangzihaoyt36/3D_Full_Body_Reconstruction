{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Mesh Segmentation.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"d0snZabugGo-","executionInfo":{"status":"ok","timestamp":1604677514684,"user_tz":-480,"elapsed":13873,"user":{"displayName":"Zihao Wang","photoUrl":"","userId":"08168071252228479816"}},"outputId":"2522fa9a-6804-4f65-a55e-0d72b8fbe194","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install tensorflow_graphics"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting tensorflow_graphics\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/60/f1e68da284a16e11db859ff2bb4ac4b8b38893e903d43d846feef6daa3d5/tensorflow_graphics-2020.5.20-py2.py3-none-any.whl (342kB)\n","\r\u001b[K     |█                               | 10kB 20.8MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 6.3MB/s eta 0:00:01\r\u001b[K     |██▉                             | 30kB 8.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 40kB 8.4MB/s eta 0:00:01\r\u001b[K     |████▊                           | 51kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 61kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 71kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 81kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 92kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 102kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 112kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 122kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 133kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 143kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 153kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 163kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 174kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 184kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 194kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 204kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 215kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 225kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 235kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 245kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 256kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 266kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 276kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 286kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 296kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 307kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 317kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 327kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 337kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 348kB 9.2MB/s \n","\u001b[?25hRequirement already satisfied: tensorflow-datasets>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_graphics) (4.0.1)\n","Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_graphics) (2.10.0)\n","Collecting OpenEXR>=1.3.2\n","  Downloading https://files.pythonhosted.org/packages/7c/c4/76bf884f59d3137847edf8b93aaf40f6257d8315d0064e8b1a606ad80b1b/OpenEXR-1.3.2.tar.gz\n","Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from tensorflow_graphics) (2.5)\n","Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_graphics) (2.3.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_graphics) (1.1.0)\n","Collecting psutil>=5.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/e0/82d459af36bda999f82c7ea86c67610591cf5556168f48fd6509e5fa154d/psutil-5.7.3.tar.gz (465kB)\n","\u001b[K     |████████████████████████████████| 471kB 16.9MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib>=2.2.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow_graphics) (3.2.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_graphics) (1.4.1)\n","Collecting trimesh>=2.37.22\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/b4/8659a500fff7a5a39dc5ce2bcb97528eb600ebd8ef203abea0df3b27f198/trimesh-3.8.12-py3-none-any.whl (626kB)\n","\u001b[K     |████████████████████████████████| 634kB 30.4MB/s \n","\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_graphics) (0.10.0)\n","Collecting tqdm>=4.45.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/3a/96b3dc293aa72443cf9627444c3c221a7ba34bb622e4d8bf1b5d4f2d9d08/tqdm-4.51.0-py2.py3-none-any.whl (70kB)\n","\u001b[K     |████████████████████████████████| 71kB 7.8MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow_graphics) (1.18.5)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics) (0.1.5)\n","Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics) (3.3.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics) (0.3.3)\n","Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics) (20.2.0)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics) (0.24.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics) (3.12.4)\n","Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics) (2.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics) (2.23.0)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics) (0.7)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics) (0.16.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics) (1.15.0)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->tensorflow_graphics) (4.4.2)\n","Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (1.1.2)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (1.12.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (1.33.2)\n","Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (0.2.0)\n","Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (1.6.3)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (3.3.0)\n","Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (2.3.0)\n","Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (2.3.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (0.35.1)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (0.3.3)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.5->tensorflow_graphics) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.5->tensorflow_graphics) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.5->tensorflow_graphics) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.5->tensorflow_graphics) (2.4.7)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from trimesh>=2.37.22->tensorflow_graphics) (50.3.2)\n","Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets>=2.0.0->tensorflow_graphics) (3.4.0)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets>=2.0.0->tensorflow_graphics) (1.52.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets>=2.0.0->tensorflow_graphics) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets>=2.0.0->tensorflow_graphics) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets>=2.0.0->tensorflow_graphics) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets>=2.0.0->tensorflow_graphics) (2.10)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2.0->tensorflow_graphics) (0.4.2)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2.0->tensorflow_graphics) (1.7.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2.0->tensorflow_graphics) (1.17.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2.0->tensorflow_graphics) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2.0->tensorflow_graphics) (3.3.3)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->tensorflow_graphics) (1.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->tensorflow_graphics) (4.6)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->tensorflow_graphics) (4.1.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->tensorflow_graphics) (0.2.8)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->tensorflow_graphics) (2.0.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->tensorflow_graphics) (3.1.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->tensorflow_graphics) (0.4.8)\n","Building wheels for collected packages: OpenEXR, psutil\n","  Building wheel for OpenEXR (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for OpenEXR: filename=OpenEXR-1.3.2-cp36-cp36m-linux_x86_64.whl size=188438 sha256=c5af8386242a04a46e0ddc1d0a58924dd2ce10ebceb6d4ba834ef075e771e700\n","  Stored in directory: /root/.cache/pip/wheels/41/06/9f/c7dc838815b0e7dfc7d7dc19cc3d677edb47594d8489adc62a\n","  Building wheel for psutil (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for psutil: filename=psutil-5.7.3-cp36-cp36m-linux_x86_64.whl size=281535 sha256=1bb94ccbe5283d77ebbc09bda2fa39975f31a60997ff4c0b7a090867d8f30ec0\n","  Stored in directory: /root/.cache/pip/wheels/42/32/da/8b12fd6b138c733efd03cfde6c6c8191a32842f9e82aa45fbf\n","Successfully built OpenEXR psutil\n","Installing collected packages: OpenEXR, psutil, trimesh, tqdm, tensorflow-graphics\n","  Found existing installation: psutil 5.4.8\n","    Uninstalling psutil-5.4.8:\n","      Successfully uninstalled psutil-5.4.8\n","  Found existing installation: tqdm 4.41.1\n","    Uninstalling tqdm-4.41.1:\n","      Successfully uninstalled tqdm-4.41.1\n","Successfully installed OpenEXR-1.3.2 psutil-5.7.3 tensorflow-graphics-2020.5.20 tqdm-4.51.0 trimesh-3.8.12\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q9SypjKmgU9A","executionInfo":{"status":"ok","timestamp":1604677526277,"user_tz":-480,"elapsed":25460,"user":{"displayName":"Zihao Wang","photoUrl":"","userId":"08168071252228479816"}},"outputId":"d46c9243-d6ee-421b-fa20-d0d99e695a47","colab":{"base_uri":"https://localhost:8080/"}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import glob\n","import os\n","%tensorflow_version 1.x%\n","import tensorflow as tf\n","\n","from tensorflow_graphics.nn.layer import graph_convolution as graph_conv\n","from tensorflow_graphics.notebooks import mesh_segmentation_dataio as dataio\n","from tensorflow_graphics.notebooks import mesh_viewer"],"execution_count":2,"outputs":[{"output_type":"stream","text":["`%tensorflow_version` only switches the major version: 1.x or 2.x.\n","You set: `1.x%`. This will be interpreted as: `1.x`.\n","\n","\n","TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9httrEjJgU_d","executionInfo":{"status":"ok","timestamp":1604677526278,"user_tz":-480,"elapsed":25458,"user":{"displayName":"Zihao Wang","photoUrl":"","userId":"08168071252228479816"}}},"source":["import numpy as np\n","from tensorflow_graphics.notebooks import threejs_visualization\n","\n","SEGMENTATION_COLORMAP = np.array(\n","    ((165, 242, 12), (89, 12, 89), (165, 89, 165), (242, 242, 165),\n","     (242, 165, 12), (89, 12, 12), (165, 12, 12), (165, 89, 242), (12, 12, 165),\n","     (165, 12, 89), (12, 89, 89), (165, 165, 89), (89, 242, 12), (12, 89, 165),\n","     (242, 242, 89), (165, 165, 165)),\n","    dtype=np.float32) / 255.0\n","\n","\n","class Viewer(object):\n","  \"\"\"A ThreeJS based viewer class for viewing 3D meshes.\"\"\"\n","\n","  def _mesh_from_data(self, data):\n","    \"\"\"Creates a dictionary of ThreeJS mesh objects from numpy data.\"\"\"\n","    if 'vertices' not in data or 'faces' not in data:\n","      raise ValueError('Mesh Data must contain vertices and faces')\n","    vertices = np.asarray(data['vertices'])\n","    faces = np.asarray(data['faces'])\n","    material = self.context.THREE.MeshLambertMaterial.new_object({\n","        'color': 0xfffacd,\n","        'vertexColors': self.context.THREE.NoColors,\n","        'side': self.context.THREE.DoubleSide,\n","    })\n","    mesh = {'vertices': vertices, 'faces': faces}\n","    if 'vertex_colors' in data:\n","      mesh['vertex_colors'] = np.asarray(data['vertex_colors'])\n","      material = self.context.THREE.MeshLambertMaterial.new_object({\n","          'color': 0xfffacd,\n","          'vertexColors': self.context.THREE.VertexColors,\n","          'side': self.context.THREE.DoubleSide,\n","      })\n","    mesh['material'] = material\n","    return mesh\n","\n","  def __init__(self, source_mesh_data):\n","    context = threejs_visualization.build_context()\n","    self.context = context\n","    light1 = context.THREE.PointLight.new_object(0x808080)\n","    light1.position.set(10., 10., 10.)\n","    light2 = context.THREE.AmbientLight.new_object(0x808080)\n","    lights = (light1, light2)\n","\n","    camera = threejs_visualization.build_perspective_camera(\n","        field_of_view=30, position=(0.0, 0.0, 4.0))\n","\n","    mesh = self._mesh_from_data(source_mesh_data)\n","    geometries = threejs_visualization.triangular_mesh_renderer([mesh],\n","                                                                lights=lights,\n","                                                                camera=camera,\n","                                                                width=600,\n","                                                                height=600)\n","\n","    self.geometries = geometries"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"h0YBD8EFgVB-","executionInfo":{"status":"ok","timestamp":1604677549501,"user_tz":-480,"elapsed":48678,"user":{"displayName":"Zihao Wang","photoUrl":"","userId":"08168071252228479816"}},"outputId":"ad9a223e-2f14-4722-be17-410206bcc9f5","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AwU4R-8EgVEa","executionInfo":{"status":"ok","timestamp":1604678344752,"user_tz":-480,"elapsed":1420,"user":{"displayName":"Zihao Wang","photoUrl":"","userId":"08168071252228479816"}}},"source":["path_to_model_zip = tf.keras.utils.get_file(\n","    'model.zip',\n","    origin='https://storage.googleapis.com/tensorflow-graphics/notebooks/mesh_segmentation/model.zip',\n","    extract=True)\n","\n","local_model_dir = os.path.join(os.path.dirname(path_to_model_zip), 'model')\n","test_data_files = [\n","    os.path.join('/content/drive/My Drive/Colab Notebooks/cgProject/new/Dancer.tfrecords')\n","]"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aqV6vkCkWB7J"},"source":["## Model Definition\n","\n","Given a mesh with V vertices and D-dimensional per-vertex input features (e.g.\n","vertex position, normal), we would like to create a network capable of\n","classifying each vertex to a part label. Let's first create a mesh encoder that\n","encodes each vertex in the mesh into C-dimensional logits, where C is the number\n","of parts. First we use 1x1 convolutions to change input feature dimensions,\n","followed by a sequence of feature steered graph convolutions and ReLU\n","non-linearities, and finally 1x1 convolutions to logits, which are used for\n","computing softmax cross entropy as described below.\n","\n","Note that this model does not use any form of pooling, which is outside the scope of this notebook.\n","\n","![](https://storage.googleapis.com/tensorflow-graphics/notebooks/mesh_segmentation/mesh_segmentation_model_def.png)"]},{"cell_type":"code","metadata":{"id":"fQVeuGazM0LK","executionInfo":{"status":"ok","timestamp":1604678344752,"user_tz":-480,"elapsed":1416,"user":{"displayName":"Zihao Wang","photoUrl":"","userId":"08168071252228479816"}}},"source":["MODEL_PARAMS = {\n","    'num_filters': 8,\n","    'num_classes': 16,\n","    'encoder_filter_dims': [32, 64, 128],\n","}\n","\n","\n","def mesh_encoder(batch_mesh_data, num_filters, output_dim, conv_layer_dims):\n","  \"\"\"A mesh encoder using feature steered graph convolutions.\n","\n","    The shorthands used below are\n","      `B`: Batch size.\n","      `V`: The maximum number of vertices over all meshes in the batch.\n","      `D`: The number of dimensions of input vertex features, D=3 if vertex\n","        positions are used as features.\n","\n","  Args:\n","    batch_mesh_data: A mesh_data dict with following keys\n","      'vertices': A [B, V, D] `float32` tensor of vertex features, possibly\n","        0-padded.\n","      'neighbors': A [B, V, V] `float32` sparse tensor of edge weights.\n","      'num_vertices': A [B] `int32` tensor of number of vertices per mesh.\n","    num_filters: The number of weight matrices to be used in feature steered\n","      graph conv.\n","    output_dim: A dimension of output per vertex features.\n","    conv_layer_dims: A list of dimensions used in graph convolution layers.\n","\n","  Returns:\n","    vertex_features: A [B, V, output_dim] `float32` tensor of per vertex\n","      features.\n","  \"\"\"\n","  batch_vertices = batch_mesh_data['vertices']\n","\n","  # Linear: N x D --> N x 16.\n","  vertex_features = tf.keras.layers.Conv1D(16, 1, name='lin16')(batch_vertices)\n","\n","  # graph convolution layers\n","  for dim in conv_layer_dims:\n","    with tf.variable_scope('conv_%d' % dim):\n","      vertex_features = graph_conv.feature_steered_convolution_layer(\n","          vertex_features,\n","          batch_mesh_data['neighbors'],\n","          batch_mesh_data['num_vertices'],\n","          num_weight_matrices=num_filters,\n","          num_output_channels=dim)\n","    vertex_features = tf.nn.relu(vertex_features)\n","\n","  # Linear: N x 128 --> N x 256.\n","  vertex_features = tf.keras.layers.Conv1D(\n","      256, 1, name='lin256')(\n","          vertex_features)\n","  vertex_features = tf.nn.relu(vertex_features)\n","\n","  # Linear: N x 256 --> N x output_dim.\n","  vertex_features = tf.keras.layers.Conv1D(\n","      output_dim, 1, name='lin_output')(\n","          vertex_features)\n","\n","  return vertex_features"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6c2pz4r_79F_"},"source":["Given a mesh encoder, let's define a model_fn for a custom\n","[tf.Estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator)\n","for vertex classification using softmax cross entropy loss. A tf.Estimator model_fn returns the ops necessary to perform training, evaluation, or predictions given inputs and a number of other parameters. Recall that the\n","vertex tensor may be zero-padded (see Dataset Pipeline above), hence we must mask out the contribution from the padded values."]},{"cell_type":"code","metadata":{"id":"WE-cuv0i78ak","executionInfo":{"status":"ok","timestamp":1604678344753,"user_tz":-480,"elapsed":1415,"user":{"displayName":"Zihao Wang","photoUrl":"","userId":"08168071252228479816"}}},"source":["def get_learning_rate(params):\n","  \"\"\"Returns a decaying learning rate.\"\"\"\n","  global_step = tf.train.get_or_create_global_step()\n","  learning_rate = tf.train.exponential_decay(\n","      params['init_learning_rate'],\n","      global_step,\n","      params['lr_decay_steps'],\n","      params['lr_decay_rate'])\n","  return learning_rate\n","\n","def model_fn(features, labels, mode, params):\n","  \"\"\"Returns a mesh segmentation model_fn for use with tf.Estimator.\"\"\"\n","  logits = mesh_encoder(features, params['num_filters'], params['num_classes'],\n","                        params['encoder_filter_dims'])\n","  predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n","  outputs = {\n","      'vertices': features['vertices'],\n","      'triangles': features['triangles'],\n","      'num_vertices': features['num_vertices'],\n","      'num_triangles': features['num_triangles'],\n","      'predictions': predictions,\n","  }\n","  # For predictions, return the outputs.\n","  if mode == tf.estimator.ModeKeys.PREDICT:\n","    outputs['labels'] = features['labels']\n","    return tf.estimator.EstimatorSpec(mode=mode, predictions=outputs)\n","  # Loss\n","  # Weight the losses by masking out padded vertices/labels.\n","  vertex_ragged_sizes = features['num_vertices']\n","  mask = tf.sequence_mask(vertex_ragged_sizes, tf.shape(labels)[-1])\n","  loss_weights = tf.cast(mask, dtype=tf.float32)\n","  loss = tf.losses.sparse_softmax_cross_entropy(\n","      logits=logits, labels=labels, weights=loss_weights)\n","  # For training, build the optimizer.\n","  if mode == tf.estimator.ModeKeys.TRAIN:\n","    optimizer = tf.train.AdamOptimizer(\n","        learning_rate=get_learning_rate(params),\n","        beta1=params['beta'],\n","        epsilon=params['adam_epsilon'])\n","    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n","    with tf.control_dependencies(update_ops):\n","      train_op = optimizer.minimize(\n","          loss=loss, global_step=tf.train.get_global_step())\n","    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n","\n","  # For eval, return eval metrics.\n","  eval_ops = {\n","      'mean_loss':\n","          tf.metrics.mean(loss),\n","      'accuracy':\n","          tf.metrics.accuracy(\n","              labels=labels, predictions=predictions, weights=loss_weights)\n","  }\n","  return tf.estimator.EstimatorSpec(\n","      mode=mode, loss=loss, eval_metric_ops=eval_ops)"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QE8Meyi5GKWA"},"source":["## Train the model from scratch\n","\n","Now let's train the mesh segmentation model from scratch. First we will download the train dataset files, and use tf.Estimator.train_and_evaluate to train a model.\n","\n","Note: Training code is provided inside colab for demonstration, and may be slow. For optimal performance, consider running the training process as a command line process, and a tensorboard process to track."]},{"cell_type":"code","metadata":{"id":"S6vxJ8t5HRcS","executionInfo":{"status":"ok","timestamp":1604678346478,"user_tz":-480,"elapsed":3137,"user":{"displayName":"Zihao Wang","photoUrl":"","userId":"08168071252228479816"}}},"source":["path_to_train_data_zip = tf.keras.utils.get_file(\n","    'train_data.zip',\n","    origin='https://storage.googleapis.com/tensorflow-graphics/notebooks/mesh_segmentation/train_data.zip',\n","    extract=True)\n","\n","train_data_files = glob.glob(\n","    os.path.join(os.path.dirname(path_to_train_data_zip), '*train*.tfrecords'))\n","\n","retrain_model_dir = os.path.join(local_model_dir, 'retrain')"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"inNSvIN-HcBg","executionInfo":{"status":"ok","timestamp":1604678346478,"user_tz":-480,"elapsed":3135,"user":{"displayName":"Zihao Wang","photoUrl":"","userId":"08168071252228479816"}},"outputId":"eb29c901-7c89-45f6-96cc-1dd4d702fac3","colab":{"base_uri":"https://localhost:8080/"}},"source":["\n","train_io_params = {\n","    'batch_size': 8,\n","    'parallel_threads': 8,\n","    'is_training': True,\n","    'shuffle': True,\n","    'sloppy': True,\n","}\n","\n","eval_io_params = {\n","    'batch_size': 8,\n","    'parallel_threads': 8,\n","    'is_training': False,\n","    'shuffle': False\n","}\n","\n","\n","def train_fn():\n","  return dataio.create_input_from_dataset(dataio.create_dataset_from_tfrecords,\n","                                          train_data_files, train_io_params)\n","\n","\n","def eval_fn():\n","  return dataio.create_input_from_dataset(dataio.create_dataset_from_tfrecords,\n","                                          test_data_files, eval_io_params)\n","\n","\n","train_params = {\n","    'beta': 0.9,\n","    'adam_epsilon': 1e-8,\n","    'init_learning_rate': 0.001,\n","    'lr_decay_steps': 10000,\n","    'lr_decay_rate': 0.95,\n","}\n","\n","train_params.update(MODEL_PARAMS)\n","\n","checkpoint_delay = 120  # Checkpoint every 2 minutes.\n","max_steps = 2000  # Number of training steps.\n","\n","config = tf.estimator.RunConfig(\n","    log_step_count_steps=1,\n","    save_checkpoints_secs=checkpoint_delay,\n","    keep_checkpoint_max=3)\n","\n","classifier = tf.estimator.Estimator(\n","    model_fn=model_fn,\n","    model_dir=retrain_model_dir,\n","    config=config,\n","    params=train_params)\n","train_spec = tf.estimator.TrainSpec(input_fn=train_fn, max_steps=max_steps)\n","eval_spec = tf.estimator.EvalSpec(\n","    input_fn=eval_fn,\n","    steps=None,\n","    start_delay_secs=2 * checkpoint_delay,\n","    throttle_secs=checkpoint_delay)\n","\n","print('Start training & eval.')\n","tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)\n","print('Train and eval done.')\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': '/root/.keras/datasets/model/retrain', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 120, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 3, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6f0c721518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","Start training & eval.\n","INFO:tensorflow:Not using Distribute Coordinator.\n","INFO:tensorflow:Running training and evaluation locally (non-distributed).\n","INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 120.\n","INFO:tensorflow:Skipping training since max_steps has already saved.\n","Train and eval done.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"94FICCro_dLV"},"source":["## Test model & visualize results\n","\n","Now that we have defined the model, let's load the weights from the trained model downloaded above and use tf.Estimator.predict to predict the part labels for meshes in the test dataset."]},{"cell_type":"code","metadata":{"id":"Olj5zIkg72FK","executionInfo":{"status":"ok","timestamp":1604678346479,"user_tz":-480,"elapsed":3132,"user":{"displayName":"Zihao Wang","photoUrl":"","userId":"08168071252228479816"}}},"source":["test_io_params = {\n","    'is_training': False,\n","    'sloppy': False,\n","    'shuffle': True,\n","    'repeat': False\n","}\n","test_tfrecords = test_data_files\n","\n","def predict_fn():\n","  return dataio.create_input_from_dataset(dataio.create_dataset_from_tfrecords,\n","                                          test_tfrecords,\n","                                          test_io_params)\n","\n","\n","test_predictions = classifier.predict(input_fn=predict_fn)\n"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IO1VmbL087xf"},"source":["Run the following cell repeatedly to cycle through the meshes in the test sequence. The left view shows the input mesh, and the right view shows the predicted part labels."]},{"cell_type":"code","metadata":{"id":"xuoVe70D5PAF","executionInfo":{"status":"ok","timestamp":1604678354134,"user_tz":-480,"elapsed":10783,"user":{"displayName":"Zihao Wang","photoUrl":"","userId":"08168071252228479816"}},"outputId":"cf62bc1b-455e-4a7e-8a99-e42bb953d815","colab":{"base_uri":"https://localhost:8080/","height":783,"output_embedded_package_id":"1G4C53zivv8480SPYHXO4NWwARXGDKEb1"}},"source":["prediction = next(test_predictions)\n","input_mesh_data = {\n","    'vertices': prediction['vertices'],\n","    'faces': prediction['triangles'],\n","}\n","predicted_mesh_data = {\n","    'vertices': prediction['vertices'],\n","    'faces': prediction['triangles'],\n","    'vertex_colors': mesh_viewer.SEGMENTATION_COLORMAP[prediction['predictions']],\n","}\n","\n","#input_viewer = mesh_viewer.Viewer(input_mesh_data)\n","#prediction_viewer = mesh_viewer.Viewer(predicted_mesh_data)\n","\n","input_viewer = Viewer(input_mesh_data)\n","prediction_viewer = Viewer(predicted_mesh_data)"],"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}